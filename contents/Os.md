# OS

**:Contents**
* [lru 알고리즘(#LRU알고리즘이란)](#LRU알고리즘이란_Least-Recently-Used)
* [프로세스와 스레드의 차이(Process vs Thread)](#프로세스와_스레드의_차이(Process vs Thread))
* [멀티 프로세스 대신 멀티 스레드를 사용하는 이유](#멀티_프로세스_대신_멀티_스레드를_사용하는_이유)
* [Thread-safe](#Thread-safe)
* [동기화 객체의 종류](#동기화_객체의_종류)
* [뮤텍스와 세마포어의 차이](#뮤텍스와_세마포어의_차이)
* [스케줄러](#스케줄러)
* [동기와 비동기](#동기와_비동기)
* [프로세스 동기화](#프로세스_동기화)
* [메모리 관리 전략](#메모리_관리_전략)
* [가상 메모리](#가상_메모리)
* [캐시의 지역성 원리](#캐시의_지역성_원리)
* [교착상태(데드락, Deadlock)의 개념과 조건](#교착상태(데드락,Deadlock)의_개념과_조건)
* [사용자 수준 스레드와 커널 수준 스레드](#사용자_수준_스레드와_커널_수준_스레드)
* [외부 단편화와 내부 단편화](#외부_단편화와_내부_단편화)
* [Swapping](#Swapping)

---

### LRU알고리즘이란  Least-Recently-Used

페이지 교체 알고리즘 중에 하나이다.
메모리에 빈 프레임이 없을 때 적재될 페이지를 위해 적재된 페이지 중 누군가는
자신이 차지한 프레임을 비워주어야 하는 교체 대상이 되어야 한다.
어떤 페이지를 선택하는 것이 최적의 기법이 인지 판단하는 다양한 기법들이 있고 페이지 교체 알고리즘이라고 한다.

LRU알고리즘은 `최근에 사용하지 않는 페이지를 가장 먼저 내려 보내는 알고리즘`이다.


`LRU알고리즘 예제`<br>


![A](imgs/os_lru.png)


* 페이지 크기가 3이라고 가정하고 0 부터 4 까지의 숫자들이 참조되는 경우를 그린 LRU 알고리즘의 상태 변화과정을 나타낸 표
* 황색 표시는 페이지가 교체 되는 상황이고 연두색은 페이지에 데이터가 존재하여 갱신되는 경우

1).시간1-6까지는 최근에 사용하지 않는 페이지를 내보내고 페이지 교체한다.
2).시간7인 경우에는 cache hit => 3
3).시간8인 경우 최근에 사용하지 않는 참조값 4를 가진 2번째 페이지 교체
4).시간9에서 다시 cache hit => 3


---


### 프로세스와 스레드의 차이(Process vs Thread)
#### 프로세스
`프로세스의 구성`<br>
![A](imgs/os_thread1.png)


- 프로세스 : 스레드 + 메모리공간(stack, code , data , heap)
- 실행중인 프로그램이다.
ex) spring boot application



[cpu 를!]
OS의 스케줄러애 의해 time slice만큼 실행

[memory 를!]
OS로 할당받은 메모리 공간 사용
code , data , heap , stack


`프로세스의 특징`<br>
- cpu는 가상메모리를 통해 Ram주소를 들어간다.

- 가상메모리는 일종의 프로세스를 가리키는 페이징 테이블이라고 한다.
프로세스간 메모리 공간격리


- 가상메모리는 프로세스간 주소공간을 논리적으로 차단한다.
>> 프로세스는 자신만의 가상 주소 공간을 가지고 있다.<br>
모든 프로세스들은 자신만의 주소 공간을 가지기 때문에, 특정 프로세스 내에서 쓰레드가 수행 될 때<br>
해당 쓰레드는 프로세스가 소유하고 있는 메모리에 대해서만 접근이 가능하다.


![A](imgs/os_multiprocess.png)


`T1 ,T2 == task_struct`<br>

task_struct는 커널의 중요한 객체이며 실행중인 프로그램인 프로세스는 리눅스에서는 테스크(task)와 동일한 의미로 사용된다.
프로세스는 파일 시스템에서 메모리로 로드한 코드, 데이터(프로그램)와 이를 커널에서 관리하기 위한 task_struct 구조체이다.


정리하자면 프로세스는 task_struct구조체를 통해 스레드라는 자원을 관리한다. 




#### 스레드
* 프로세스를 만드는 것보다 비용이 저렴
* 스레드들간 HEAP 메모리 공간 공유
* 하나의 프로세스에 하나의 스레드가 있는 상황에서 신규 스레드가 추가된다면 신규 스레드가 사용할 stack 메모리 공간만
생성된다.



![A](imgs/os_newthread.png)


---


### 멀티 프로세스 대신 멀티 스레드를 사용하는 이유

[리눅스기준]
#### 스레드의 생성의 비용이 더 저렴하다.


#### context switch 비용

cpu와 LAM 영역 사이에 메모리 인풋 아웃 IO작업이 일어나는데
LAM 영역에 접근하는데 많이 시간이 걸린다.

스레드의 경우 HEAP 메모리 영역을 공유하므로 한 프로세스안에 다른 스레드들이 context switch이
일어나도 캐시에 같이 데이터가 있을 수 있기 때문에

캐싱 측면에서 스레드 모델 성능이 좋다.


멀티 프로세스 모델은 서로간 메모리를 공유 할 수 없기 때문에 
context switch시 캐시 레이어를 리셋해버린다.
그렇기 때문에 속도적으로 이슈가 생긴다.



![A](imgs/os_io_time.png)

![A](imgs/os_process-thread.png)


---

### Thread-safe
* 멀티 스레드 프로그래밍에서 함수, 변수, 객체 등이 여러 스레드로부터 동시에 접근이 이루어져도
프로그램의 실행에 문제가 없다는 것을 뜻한다.   
* 하나의 함수가 한 Thread로부터 호출되어 실행 중일 때
다른 Thread가 그 함수를 호출하여 동시에 함께 실행되더라도 각 스레드에서의 함수의 수행 결과가 의도한대로 나와야합니다.

* Thread-safe를 지키기 위한 방법
    1. Re-entrancy : 어떤 함수가 한 스레드에 의해 호출되어 실행 중일 때, 
    다른 스레드가 그 함수를 호출하더라도 그 결과가 각각에게 올바로 주어져야 한다.
    2. Thread-local storage : 공유 자원의 사용을 최대한 줄여 각각의 스레드에서만 접근 가능한 저장소들을 사용함으로써 동시 접근을 막는다.
    이 방식은 동기화 방법과 관련되어 있고, 또한 공유상태를 피할 수 없을 때 사용하는 방식이다.
    3. Mutual exclusion : 공유 자원을 꼭 사용해야 할 경우 해당 자원의 접근을 세마포어 등의 락으로 통제한다.
    4. Atomic operations : 공유 자원에 접근할 때 원자 연산을 이용하거나 '원자적'으로 정의된 접근 방법을 사용함으로써 상호 배제를 구현할 수 있다.
 
* Thread Safe한 코드 작성하기
    1. Synchronized 함수 사용    
    -클래스로 만들어진 하나의 인스턴스를 기준으로 동기화가 이루어진다.
    2. Synchronized 블럭 사용    
    -전달받은 객체를 기준으로 동기화가 이루어진다. 
    3. ReentrantLock 사용    
    -시작점과 끝점을 명백히 명시할 수 있다. Synchronized는 암묵적이고 ReentrantLock는 명시적이라는 차이가 있다.
    4. 세마포어 사용    
    -하나의 스레드만 임계구역에 들어가면 성능 이슈가 발생하는데, 세마포어는 임계구역에 여러 스레드가 들어갈 수 있는 장점이 있다.
     공유자원이 2개 이상일 때 잘못 사용하면 서로 자원을 점유하기 위해서 대기상태에 빠지므로 DeadLock이 발생할 수 있다.
    5. 모니터 사용    
    -모니터는 2개의 Queue가 존재한다.     
     하나의 Queue는 하나의 Thread만 공유자원에 접근할 수 있게하는 역할을 한다.(상호배타)     
     다른 Queue는 임계구역에 진입한 wait()을 통해 Thread가 블락되면 새로운 Thread가 진입할 수 있도록 알려주는 역할을 한다.(조건동기)     
     그리고 새로 진입한 Thread가 notify()를 통해 블락된 Thread를 재진입할 수 있도록 하는 역할도 한다.(조건동기)    
     *cf ) 임계 구역 : 파일, 입출력, 공유 데이터 등 원자적으로 실행할 필요가 있는 명령문 또는 코드의 일부 영역
     * cf Link ) [Thread-safe을 위한 방법](https://www.goozy.net/6)
---     
### 동기화 객체의 종류 
#### Critical section 이란 ?
* 동기화 방법 중 유일하게 커널 객체를사용하지 않으며 그 내부 구조가 단순하기 때문에 동기화 처리를 하는 데 있어서 속도가 빠르다는 장점이 있으며 동일한 프로세스내에서만 사용할 수 있다는 제약이 있다.    
* 크리티컬 섹션은 커널 객체를 사용하지 않기 때문에 핸들을 사용하지 않고 대신 CRITICAL_SECTION라는 타입을 정의하여 사용하게 되며, 아래 4가지 Win32 API를 사용하여 동기화를 수행하게 된다.    
    * VOID InitializeCriticalSection(LRCRITICAL_SECTION IpCriticalSection);
      VOID DeleteCriticalSection(LRCRITICAL_SECTION IpCriticalSection);
      VOID EnterCriticalSection(LRCRITICAL_SECTION IpCriticalSection);
      VOID LeaveCriticalSection(LRCRITICAL_SECTION IpCriticalSection);

* ★★★ 공유 데이타를 여러 프로세스가 동시에 액세스하면 시간적인 차이 때문에 잘못된 결과를 만들어 낼 수 있기 때문에 한 프로세스가 위험 부분을 수행하고 있을 때, 즉 공유 데이타를 액세스하고 있을 때는 다른 프로세스들은 절대로 그 데이타를 액세스하지 못하도록 하여야 한다.
* ※ 커널 객체를 사용하지 않는 동기화 객체는 크리티컬 섹션뿐이며 뮤택스를 포함한 동기화 객체들, 스레드(Thread), 파일(File)들까지도 동기화를 위한 커널 객체를 포함하고 있다.

#### 이벤트란(Event)?
* 이벤트는 어떠한 사건에 대하여 알리기 위한 용도로 사용되는 동기화 객체이다. 이벤트 객체의 경우 우리가
Windows에서 메시지를 교환할 때 사용하는 개념과 같은 개념이다. 즉, Windows의 메시지 교환 방식은 이벤트에
기반한 방식이라고 이야기한다. 키보드가 눌려질 경우 WM_KEYDOWN, 마우스 왼쪽 버튼이 눌러질 경우
WM_LBUTTONDOWN과 같은 메시지가 날라오며, 이러한 신호는 이벤트를 통하여 통보하게 되는 것이다.
* 이벤트의 경우 SetEvent() 또는 ResetEvent()를 사용하여 동기화 객체들의 상태를 마음대로 바꿀 수 있게 된다.

#### 뮤텍스란(Mutex)? 
* 공유된 자원의 데이터를 여러 쓰레드가 접근하는 것을 막는 것
* Mutual Exclusion 으로 상호배제라고도 한다. 
 Critical Section을 가진 쓰레드들의 Runnig Time이 서로 겹치지 않게 각각 단독으로 실행되게 하는 기술입니다. 다중 프로세스들의 공유 리소스에 대한 접근을 조율하기 위해 locking과 unlocking을 사용한다. 
 즉, 쉽게 말하면 뮤텍스 객체를 두 쓰레드가 동시에 사용할 수 없다는 의미입니다.    
* 뮤텍스는 스레드가 여러 개 있더라도 자신이 소유한 스레드가 누구인지를 기억하고 있으며, Windows 운영체제는 뮤텍스가 반환되지 않은 상태에서 스레드가 종료될 경우 그 뮤텍스를 강제적으로 Signaled 해줌으로써 이를 대기하고 있던 스레드가 무한정 기다리는 일이 없도록 해준다.    
* 뮤텍스는 뮤텍스를 소유한 스레드를 기억하고 있음으로써 같은 스레드가 같은 뮤텍스를 중복 호출하더라도 데드락 현상이 발생하기 않게 하고있다.    
즉, 내부적으로 같은 스레드가 같은 뮤텍스를 소유하려 할 경우 뮤텍스의 내부적인 카운트만을 증가시켜 주고 이 스레드에 대한 진입은 허용하여 주게 된다.     
그리고 이 내부적인 카운트가 0으로 내려갔을 경우에 Signaled 상태로 돌려줌으로써 다른 스레드에 대해서는 중복 호출한 스레드가 뮤텍스에 대한 사용을 모두 종료한 후에 진입하도록 하고 있으며, 이러한 기능은 같은 스레드의 재귀 호출에 의한 데드락 현상을 방지하여 주게 된다.
  
![A](imgs/mutex.png)
  
#### 세마포어란?(Semaphore) 
* 공유된 자원의 데이터를 여러 프로세스가 접근하는 것을 막는 것
* 세마포어는 사용자가 지정한 개수만큼 이 동기화 객체로 보호하는 자원에 대하여 접근할 수 있도록 하고 있다.
즉, 세마포어에서는 사용 가능한 자원의 개수를 세팅할 수 있도록 하고 있으며, 그 값은 세마포어 초기화 시에 세팅하게 된다.    
* 세마포어는 리소스의 상태를 나타내는 간단한 카운터로 생각할 수 있다.    
일반적으로 비교적 긴 시간을 확보하는 리소스에 대해 이용하게 되며, 유닉스 시스템의 프로그래밍에서 세마포어는 운영체제의 리소스를 경쟁적으로 사용하는 다중 프로세스에서 행동을 조정하거나 또는 동기화 시키는 기술이다.   
세마포어는 운영체제 또는 커널의 한 지정된 저장장치 내 값으로서, 각 프로세스는 이를 확인하고 변경할 수 있다. 확인되는 세마포어의 값에 따라, 그 프로세스가 즉시 자원을 사용할 수 있거나, 또는 이미 다른 프로세스에 의해 사용 중이라는 사실을 알게 되면 재시도하기 전에 일정 시간을 기다려야만 한다. 세마포어는 이진수 (0 또는 1)를 사용하거나, 또는 추가적인 값을 가질 수도 있다.   
세마포어를 사용하는 프로세스는 그 값을 확인하고, 자원을 사용하는 동안에는 그 값을 변경함으로써 다른 세마포어 사용자들이 기다리도록 해야한다.

![A](imgs/semaphore.png)


---

### 뮤텍스와 세마포어의 차이
#### Mutex & Semaphore 차이
1) Semaphore는 Mutex가 될 수 있지만 Mutex는 Semaphore가 될 수 없습니다.(Mutex 는 상태가 0, 1 두 개 뿐인 binary Semaphore)
2) Semaphore는 소유할 수 없는 반면, Mutex는 소유가 가능하며 소유주가 이에 대한 책임을 집니다. (Mutex 의 경우 상태가 두개 뿐인 lock 이므로 lock 을 ‘가질’ 수 있습니다.)
3) Mutex의 경우 Mutex를 소유하고 있는 쓰레드가 이 Mutex를 해제할 수 있습니다. 하지만 Semaphore의 경우 이러한 Semaphore를 소유하지 않는 쓰레드가 Semaphore를 해제할 수 있습니다. 
4) Semaphore는 시스템 범위에 걸쳐있고 파일시스템상의 파일 형태로 존재합니다. 반면 Mutex는 프로세스 범위를 가지며 프로세스가 종료될 때 자동으로 Clean up된다.
* ★★★ 가장 큰 차이점은 관리하는 동기화 대상이 갯수다. Mutex는 동기화 대상이 오직 하나뿐일 때, Semaphore는 동기화 대상이 하나 이상일 때 사용한다.

---

### 스케줄러(Scheduler)란?
#### 스케줄러 
프로세를 스케줄링하기 위한 Queue에는 세 가지 종류가 존재한다.
>+ __Job Queue__ : 현재 시스템 내에 있는 모든 프로세스의 집합
>+ __Ready Queue__ : 현재 메모리 내에 있으면서 CPU를 잡아 실행되기를 기다리는 프로세스의 집합
>+ __Device Queue__ : Device I/O 작업을 대기하고 있는 프로세스의 집합

##### 장기 스케줄러(Long-term scheduler or job scheduler)
메모리는 한정되어 있는데 여러 프로세스들이 한 번에 메모리에 올라올 경우에 대용량 메모리(일반적으로 디스크)에 임심로 저장된다. 이 Pool에 저장되어 있는 프로세스 중 어떤 프로세스에 메모리를 할당하여 Ready Queue 로 보낼지 경정하는 역할을 한다.
>+ 메모리와 디스크 사이의 스케줄링을 담당
>+ 프로세스에 메모리(및 각종 리소스)를 할당(admit)
>+ degree of multiprogramming 제어  
(메모리에 여러 프로그램이 올라가는 것) 몇 개의 프로그램이 올라갈 것인지를 제어
>+ 프로세스의 상태  
new => ready(in memory)  

##### 중기 스케줄러(Medium-term scheduler or Swapper)
>+ 여유 공간 마련을 위해 아직 실행하지 않아도 되는 프로세스를 통째로 메모리에서 제거하고 디스크로 쫓아냄 (__Swapping__)
>+ Performance (속도)는 느려지지만 여러 프로세스르 실행시키기 위해서는 반드시 필요한 작업  

##### 단기 스케줄러(short-term scheduler or CPU scheduler)
>+ CPU 와 메모리사이의 스케줄링을 담당
>+ 굉장히 자주 돌며 Ready Queue 에 존재하는 프로세스 중 어떤 프로세스를 running 시킬 것인지 결정
>+ 시간을 엄청 많이 나누어 그 순간마다 여러 프로세스를 실행시키는 즉, 멀티태스킹을 하게 해주는 스케줄러  

![A](imgs/os_scheduler.png)
---

### 동기와 비동기
#### 동기(Sync) 와 비동기(Async)
![A](imgs/os_sync_io_model.png)  
<Synchronous I/O Model>  
  
Sync와 Async 를 구분하는 기준은 작업순서이다.  
동기식 모델은 모든 작업들이 일련의 순서를 따르며 그 순서에 맞게 동작한다. 즉, A,B,C 순서대로 작업이 시작되었다면 A,B,C 순서로 작업이 끝나야 한다. 설령 여러 작업이 동시에 처리되고 있다고 해도, 작업이 처리되는 모델의 순서가 보장된다면 이는 동기식 처리 모델이라고 할 수 있다.  
  
많은 자료들이 동기식 처리 모델을 설명할 때, 작업이 실행되는 동안 다음 처리를 기다리는 것이 (Wait) Sync 모델이라고도 하지만, 이는 잘 알려진 오해이다. 이 Wait Process 때문에 Blocking 과 개념의 혼동이 생기는 경우가 흔하다.  
동기 처리 모델에서 알아두어야할 점은 __작업의 순서가 보장된다는 점__ 뿐이다.

![A](imgs/os_async_io_model.png)  
<Asynchronous I/O Model>  
  
반면 비동기식 모델은 작업의 순서가 보장되지 않는다. 말 그대로 비동기(Asynchronous) 처리 모델로 A, B, C 순서로 작업이 시작되어도 A, B, C 순서로 작업이 끝난다고 보장할 수 없다.  
비동기식 처리 모델이 이득을 보는 경우는 각 작업이 분리될 수 있으며, Latency 가 큰 경우이다. 예를 들면 각 클라이언트 또는 작업 별로 Latency 가 발생하는 네트워크 처리나 File I/O 등이 훌륭한 적용 예시인 것


---
### 프로세스 동기화
#### 병행 프로세스(Concurrent Process)의 문제
>+ 병행 프로세스(Concurrent Process)는 두 개 이상의 프로세스들이 동시에 존재하여 실행 상태에 있는 것을 의미
>+ 컴퓨터 자원에는 어느 한 시점에 하나의 프로세스가 할당되어 수행되는데, 동시에 두 개 이상의 프로세스를 병행 처리하면 여러 문제점(race condition 과 같은)이 발생될 수 있다.  
이러한 문제를 해결하기 위해 임계 영역, 상호 배제 기법, 동기화 기법 등을 사용하는 것  

#### 임계 구역(Critical Section)
임계 구역(Critical Section)은 여러 개의 프로세스가 공유하는 데이터 및 자원에 대하여 어느 한 시점에서는 하나의 프로세스만 사용할 수 있도록 지정(공유 자원의 독점을 보장)하는 것을 의미
>+ 임계 구역에는 하나의 프로세스만 접근가능하며, 반납 후에만 다른 프로세스가 사용 가능
>+ 임계 구역은 특정 프로세스가 독점할 수 없으며, 임계 영역에서 수행 중인 프로세스는 인터럽트가 불가능
>+ 임계 구역의 자원, 데이터는 여러 프로세스가 사용해야 하므로 임계 구역 내에서의 작업은 신속하게 이루어져야 함  


#### 상호 배제 기법(Mutual Exclusion)
상호 배제(Mutual Exclusion)는 특정 프로세스가 공유 자원을 사용하고 있을 경우 다른 프로세스가 해당 공유 자원을 사용하지 못하게 제어하는 기법을 의미
>+ 공유 자원을 사용할 때 여러 프로세스가 번갈아가며 공유 자원을 사용하도록 하여 임계 구역을 유지하는 기법
>+ 상호 배제 기법을 구현하기 위한 방법은 소프트웨어적 구현과 하드웨어적 구현이 있다.
>>- 소프트웨어적 구현 방법 :  
데커(Dekker) 알고리즘, 피터슨(Perterson) 알고리즘 => 2개의 프로세스 기준  
Lamport의 빵집 알고리즘 => 여러 개의 프로세스 기준

>>- 하드웨어적 구현 방법 :  
Test & Set 기법과 Swap 명령어 기법이 있다.


#### 동기화 기법(Synchronization)
동기화 기법(Synchronization)은 두 개 이상의 프로세스를 한 시점에서는 동시에 처리할 수 없으므로 각 프로세스에 대한 처리 순서를 결정하는 것으로, 상호 배제의 한 형태
>+ 동기화를 구현할 수 있는 방법으로는 세마포어(Semaphore)와 모니터(Monitor)가 있다.

#### 세마포어(Semaphore)
>+ '신호기', '깃발'을 뜻하며, 각 프로세스에 제어 신호를 전달하여 순서대로 작업을 수행하도록 하는 기법
>+ 다익스트라가 제안했으며, P와 V라는 두 개의 연산에 의해 동기화를 유지시키고 상호 배제의 원리를 보장한다.
>>+ S 는 P 와 V 연산으로만 접근 가능한 세마포어 변수로, 공유 자원의 개수를 나타냄  
P 연산 : 자원을 사용하려는 프로세스들의 진입 여부를 자원의 개수(S)를 통해 결정하는 것으로 Wait 동작이라고 함  
S = S-1 : 자원 점유를 알리는 것으로 자원의 개수를 감소시킨다.
V 연산 : 대기 중인 프로세스를 깨우는 신호(Wake Up)로서, Signal 동작이라 한다.
S = S+1 : 자원 반납을 의미하므로 자원의 개수를 증가 시킨다.  
  
정리하자면 세마포어(S)는 프로세스가 임계 구역에 들어가려할 때(wait) 값이 감소하고, 임계 구역의 작업을 끝내고 반납할 때(signal) 값이 증가  
만약 세마포어가 0이 된다면 모든 자원들이 프로세스들에 의해 사용중이라는 것을 나타내며, 자원을 사용하기 위해서는 세마포어가 0보다 커지기를 기다려야 함 


#### 모니터(Monitor)
모니터(Monitor)는 동기화를 구현하기 위한 특수 프로그램 기법으로 특정 공유 자원을 프로세스에게 할당하는데 필요한 데이터와 이 데이터를 처리하는 프로시저로 구성
사실 세마포어(Semaphore)는 오래된 동기화 도구라고 할 수 있다. 그래서 현재 사용되는 도구 중 하나가 모니터이며 특히 자바 프로그램에서는 모니터에 대한 활용이 높다.
  
모니터의 경우 두 개의 Queue 가 있는데 각각 배타동기와 조건동기의 역할을 한다.  
배타동기의 Queue 는 하나의 스레드(thread)만 공유자원에 접근할 수 있게 하는 공간이다. 특정 스레드(thread)가 공유 자원을 사용하는 함수를 사용하고 있으면 다른 스레드(thread)는 대기해야 한다.  
조건동기의 Queue 는 진입 스레드(thread)가 블록되면서 새 스레드(thread)가 진입가능하게 하는 공간이다. 새 스레드(thread)는 조건동기로 블록된 스레드(thread)를 깨울 수 있다. 깨워진 스레드(thread)는 현재 스레드(thread)가 나가면 재진입할 수 있다.  
  
자바의 모니터를 보며 이해해보자.
![A](imgs/os_monitor1.png)
![A](imgs/os_monitor2.png)
![A](imgs/os_monitor3.png)  

자바의 모든 객체는 모니터가 될 수 있다. 배타 동기는 synchronized 키워드를 통해 지정할 수 있다.  
조건 동기는 wait(), notify(), notifyAll() 함수를 사용한다. 배타 동기를 선언해주는 Synchronized 는 적어주는 것만으로도 상호 배제 원리를 만족시키는 함수로 만들어준다.
조건 동기의 경우 wait() 함수를 실행하면 진입 쓰레드를 조건 동기 Queue 에 블록 시키며, notify() 함수는 그렇게 블록된 함수를 새로운 스레드(thread)가 실행하는 방식으로 깨우게 되고, notifyAll()은 모든 스레드(thread)를 깨우는 것으로 사용할 수 있다. 

---

### 메모리 관리 전략

>> 제한된 물리 메모리의 효율적인 사용과 메모리 참조 방식을 제공하기 위해 필요

#### 연속 메모리 할당

- 프로세스를 메모리에 연속적으로 할당하는 기법
- 할당과 제거를 반복하다보면 Scattered Holes가 생겨나고 이로 인한 외부 단편화가 발생


`Scattered Holes`<br>
![A](imgs/os_hole.png)


- 연속 메모리 할당에서 외부 단편화를 줄이기 위한 방식 3가지

`최초 적합(First-fit)`<br>
메모리를 순차적으로 탐색하여 제일 먼저 발견한 적절하게 들어갈 수 있는 곳을 찾아 프로세스를 적재하는 방법이다.



`최적 적합(Best-fit)`<br>
메모리를 탐색하여 메모리 공간 중에서 제일 적절하게 들어갈 수 있는 곳을 찾아 프로세스를 적재하는 방법이다.<br>
메모리 공간의 크기와 프로세스의 크기 차이가 제일 적은 경우




`최악 적합(Worst-fit)`<br>
메모리에 넣는데 크기와 제일 안 맞는 공간(프로세스보다 큰 메모리 공간 중에서)에 프로세스를 넣는 방식이다.



#### 페이징

- 패이징이란 논리주소의 고정된 페이지라고 불리는 블록들로 분할 관리하는 기법
- 각각의 페이지는 물리 메모리의 프레임과 맵핑
- 페이지를 가리키는 논리주소에서 프레임을 가리키는 물리주소로 변환

>> 결론은 외부 단편화를 보완하기 위해 사용


1.logical address를 동일한 크기로 자름(고정 분할)<br>
2.physical address도 이것과 동일한 크기로 자름<br>
3.A,B프로세스의 고정 분할된 일부의 page는 물리메모리의 어디에 배치되고 어디에 배치되고.... 계속 하게 되면 외부 단편화가 발생하지 않음<br>

![A](imgs/os_paging.PNG)


#### 세그멘테이션

- 페이징기법과 반대로 논리 메모리와 물리 메모리를 같은 크기가 아닌 다른 크기의 `논리적 단위인 세크멘트로` 분할
- 세그먼트의 크기는 일반적으로 같지 않다.

ex).우리가 한우를 잡아서 보관한다면 페이징의 경우는 모든 부위를 `같은 단위`로 잘라 보관하는 것이고<br>
세그멘테이션 기법은 채끝,꽇등심등 부위별로 잘라 보관하는 것이다.

>> 메모리를 자르는 방법을 빼고 메모리에 힐당하는 방법은 페이징 기법과 방 같다.


![A](imgs/os_segment.png)



#### 세그멘테이션 페이징 혼용 기법

- 페이징과 세그멘테이션도 각각 내부 단편화와 외부 단편화가 발생

- 페이징과 세그멘테이션을 혼용해 이러한 단편화를 최대한 줄이는 전략

- 프로세스를 세그먼트(논리적 기능 단위)로 나눈 다음 세그먼트를 다시 페이지 단위로 나누어 관리

- 매핑 테이블을 두 번 거쳐야하므로 속도가 느려짐

---

### 가상 메모리

가상 메모리 기법은 프로그램 전체가 아닌 필요한 일부부만 실제 메모리에 올리는 기업입니다.
>> 실제 사용하는 메모리는 작다는 점에서 고안된 기술 

즉, 가상메모리는 프로세스의 물리 메모리와 논리 메모리를 분리하기 위해 생겨난 것입니다.


`가상 메모리를 사용하는 이유`

>> 각 프로세스마다 충분한 메모리를 할당하여 사용하기에는 메모리 크기에 한계가 있다.

>> 프로세스간 메모리 영역간 침범 방지


'가상메모리가 없는 경우'<br>

RAM의 메모리가 4GB라고 하고, 프로세스 A,B에 필요한 메모리가 4GB이라면<br>
메모리에 프로세스 A가 먼저 할당이 된다면 , 프로세스 B는 할당받을 메모리가 부족하여 사용할 수 없다. 

![A](imgs/os_vi_memory1.png)


'가상메모리가 있는 경우'<br>


RAM의 메모리가 4GB라고 하고, 프로세스 A,B,C가 있다고 한다면,<br>
프로세스가 현재 사용되는 메모리 만큼만 물리 메모리(RAM)에 할당과 해제를 반복하여<br>
메모리를 사용한다면 여러 프로세스가 사용 할 수 있다.

![A](imgs/os_vi_memory2.png)




---

### 캐시의 지역성 원리

캐시 메모리는 속도가 빠른 장치와 느린 장치간의 속도차에 따른 병목 현상을 줄이기 위한 범용 메모리이다.<br>
이러한 역할을 수행하기 위해서는 CPU 가 어떤 데이터를 원할 것인가를 어느 정도 예측할 수 있어야 한다.

- 캐시 메모리에서 원하는 데이터를 찾는다면 메인 메모리까지 가서 찾지 않아도 되기 때문에 성능 향상
- 캐시 메모리에 원하는 데이터에 적중률 성능의 관건


>> 이때 적중율을 높이기 위해 데이터 지역성 원리를 사용한다고 한다.

`시간 지역성`<br>
- 최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성

ex)<br>
for나 while 같은 반복문에 사용하는 조건 변수처럼 한번 참조된 데이터는 잠시 후에 또 참조될 가능성이 높다

`공간 지역성`<br>
- 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성

캐시 메모리에 데이터를 저장할 때 공간 지역성을 최대한 활용하기 위해<br>
해당 데이터뿐만 아니라 옆 주소의 데이터도 같이 가져와 미래에 쓰일 것을 대비한다.


ex)<br>
A[0], A[1]과 같은 데이터 배열에 연속으로 접근할 때 참조된 데이터 근처에 있는 데이터가 잠시 후에 사용될 가능성이 높다


---

### 교착상태(데드락, Deadlock)의 개념과 조건
### DeadLock의 개념    
-프로세스가 자원을 얻지 못해 다음 처리를 하지 못하는 상태로, ‘교착 상태’라고도 하며 시스템적으로 한정된 자원을 여러 곳에서 사용하려고 할 때 발생합니다.
-데드락이 발생할 수 있는 경우 :    
![A](imgs/os_deadlock.png)  
P1과 P2가 리소스 A, B 둘 다를 얻어야 한다고 가정할 때,
t1에 P1이 리소를 A를 얻고 P2가 리소스 B를 얻었다면 t2때 P1은 리소스 B를, P2는 리소스 A를 기다리게 됩니다.
하지만 서로 원하는 리소스가 상대방에게 할당되어 있기 때문에 이 두 프로세스는 무한정 기다리게 되는데 이러한 상태을 DeadLock상태라고 합니다.    
- 발생되는 상황  
    - 멀티 프로그래밍 환경에서 한정된 자원을 사용하려고 서로 경쟁하는 상황이 발생 할 수 있습니다.
    어떤 프로세스가 자원을 요청 했을 때 그 시각에 그 자원을 사용할 수 없는 상황이 발생할 수 있고 그 때는 프로세스가 대기 상태로 들어 가게됩니다.
    대기 상태로 들어간 프로세스들이 실행 상태로 변경 될 수 없을 때 이러한 상황을 교착 상태라 합니다.    

### 데드락 (Dead lock)의 발생 조건
- 교착 상태는 한 시스템 내에서 다음의 네 가지 조건이 동시에 성립 할 때 발생합니다
- 따라서, 아래의 네 가지 조건 중 하나라도 성립하지 않도록 만든다면 교착 상태를 해결할 수 있습니다.    
    1) 상호 배제 (Mutual exclusion)    
    -자원은 한 번에 한 프로세스만이 사용할 수 있어야 한다.
    2) 점유 대기 (Hold and wait)    
    -최소한 하나의 자원을 점유하고 있으면서 다른 프로세스에 할당되어 사용하고 있는 자원을 추가로 점유하기 위해 대기하는 프로세스가 있어야 한다.
    3) 비선점 (No preemption)    
    -다른 프로세스에 할당된 자원은 사용이 끝날 때까지 강제로 빼앗을 수 없어야 한다.
    4) 순환 대기 (Circular wait)    
    -프로세스의 집합 {P0, P1, ,…Pn}에서 P0는 P1이 점유한 자원을 대기하고 P1은 P2가 점유한 자원을 대기하고 P2…Pn-1은 Pn이 점유한 자원을 대기하며 Pn은 P0가 점유한 자원을 요구해야 한다.

### 데드락 (Dead lock) 처리    
  1. 교착 상태 예방(Prevention) 및 회피(Avoidance)    
    <예방(Prevention)법> : 교착 상태 발생 조건 중 하나를 제거함으로써 해결하는 방법 - 자원의 낭비가 심하다.    
    <회피(Avoidance)법> : 교착 상태가 발생하면 피해나가는 방법    
    - 은행원 알고리즘 (Banker’s Algorithm)    
    E,J,Dijkstra가 제안한 방법으로, 은행에서 모든 고객의 요구가 충족되도록 현금을 할당하는 데서 유래한 기법이다.프로세스가 자원을 요구할 때 시스템은 자원을 할당한 후에도 안정 상태로 남아있게 되는지를 사전에 검사하여 교착 상태를 회피하는 기법 
    안정 상태에 있으면 자원을 할당하고, 그렇지 않으면 다른 프로세스들이 자원을 해지할 때까지 대기함
    교착 상태가 되지 않도록 보장하기 위하여 교착 상태를 예방하거나 회피하는 프로토콜을 이용하는 방법    

  2. 교착 상태 탐지 및 회복 : 교착 상태가 되도록 허용한 다음에 회복시키는 방법 - 교착 상태 무시    
    -** 대부분의 시스템은 교착 상태가 잘 발생하지 않으며, 교착 상태 예방, 회피, 탐지, 복구하는 것은 비용이 많이 든다. **    
>>    <교착 상태 탐지 (Detection)>    
        - 자원 할당 그래프를 통해 교착 상태를 탐지할 수 있다.
        - 자원을 요청할 때마다 탐지 알고리즘을 실행하면 그에 대한 오버헤드가 발생한다    
>>    <교착 상태로부터 회복 (Recovery)>
        - 교착 상태를 일으킨 프로세스를 종료하거나, 할당된 자원을 해제함으로써 회복하는 것을 의미한다.
        - 프로세스를 종료하는 방법    
            1. 교착 상태의 프로세스를 모두 중지
            2. 교착 상태가 제거될 때까지 한 프로세스씩 중지
        - 자원을 선점하는 방법    
            1. 교착 상태의 프로세스가 점유하고 있는 자원을 선점하여 다른 프로세스에게 할당하며, 해당 프로세스를 일시 정지 시키는 방법
            2. 우선 순위가 낮은 프로세스, 수행된 횟수가 적은 프로세스 등을 위주로 프로세스의 자원을 선점한다.

출처: https://jwprogramming.tistory.com/12 

### 사용자 수준 스레드와 커널 수준 스레드
사용자 레벨 스레드는 말그대로 우리가 #include <thread> 혹은 import를 통해 스레드를 이용하는 것을 의미한다.
![A](imgs/os_user_kernel_levl.png)
커널 레벨 스레드는 커널 내에 있는 스레드를 의미하게 되고 위와같이 3가지 방법으로 나뉜다.
  1. Pure user-level    
    커널 스레드 1개당 사용자 스레드 n개를 의미한다. 즉, 1 : n 방식이다.    
    이 방식같은 경우에는 커널은 사용자 스레드가 100개가 있어도 전혀 모르기 때문에 사용자 스레드에서 I/O가 하나라도 발생하면 해당 프로세스는 I/O가 풀릴 때 까지 영원히 block된다.
  2. Pure Kernel-level    
    n개의 커널 스레드가 n개의 사용자 스레드를 담당하게 된다. 즉 1:1 방식이다.
    1:1 방식이기에 병렬성은 좋으나 효율성 면에서 다소 떨어진다.
  3. Combined   
    커널 스레드와 사용자 스레드를 혼합하여 사용하는 방식이다. 위의 두 방식의 장점을 혼합한 방식이라 생각 할 수 있다.

#### 커널 레벨 스레드
- 커널 스레드는 가장 가벼운 커널 스케쥴링 단위다. 
- 하나의 프로세스는 적어도 하나의 커널 스레드를 가지게 된다. 
- 커널 영역에서 스레드 연산을 수행하게 된다.
- 커널이 스레드를 관리하기 때문에 커널에 종속적이다.
- 프로그래머 요청에 따라 스레드를 생성하고 스케줄링하는 주체가 커널이면 커널 레벨(Kernel Level) 스레드라고 한다.

#### 커널 레벨 스레드 장점
- 프로세스의 스레드들을 몇몇 프로세서에 한꺼번에 디스패치 할 수 있기 때문에 멀티프로세서 환경에서 매우 빠르게 동작한다.
- 다른 스레드가 입출력 작업이 다 끝날 때까지 다른 스레드를 사용해 다른 작업을 진행할 수 있다. 
- 커널이 각 스레드를 개별적으로 관리할 수 있다. 
- 커널이 직접 스레드를 제공해 주기 때문에 안정성과 다양한 기능이 제공된다.

#### 커널 레벨 스레드 단점
- 스케줄링과 동기화를 위해 커널을 호출하는데 무겁고 오래걸린다.(저장한 내용을 다시 불러오는 과정이 필요)
- 즉, 사용자 모드에서 커널 모드로의 전환이 빈번하게 이뤄져 성능 저하가 발생한다.
- 사용자가 프로그래밍할 때 구현하기 어렵고 자원을 더 많이 소비하는 경향이 있다.

#### 사용자 레벨 스레드
- 사용자 영역에서 스레드 연산을 수행한다. 
- 사용자 영역에서 스레드 연산을 수행하기 때문에 운영체제에 투명하다. 
- 커널에 의존적이지 않은 형태로 스레드의 기능을 제공하는 라이브러리를 활용하는 방식이 사용자 레벨(User Level) 스레드다.

#### 사용자 레벨 스레드 장점
- 운영체제에서 스레드를 지원할 필요가 없다. 
- 스케줄링 결정이나 동기화를 위해 커널을 호출하지 않기 때문에 인터럽트가 발생할 때 커널 레벨 스레드보다 오버헤드가 적다.
- 즉, 위의 말은 사용자 영역 스레드에서 행동을 하기에 OS Scheduler의 context switch가 없다(유저레벨 스레드 스케줄러를 이용).
- 커널은 사용자 레벨 스레드의 존재조차 모르기 때문에 모드 간의 전환이 없고 성능 이득이 발생한다

#### 사용자 레벨 스레드 단점
- 시스템 전반에 걸친 스케줄링 우선순위를 지원하지 않는다. (무슨 스레드가 먼저 동작할 지 모른다.)
- 프로세스에 속한 스레드 중 I/O 작업등에 의해 하나라도 블록이 걸린다면 전체 스레드가 블록된다.

cf)    
-커널?    
    운영체제의 다른 모든 부분에 여러 기본적인 서비스를 제공하고 컴퓨터 하드웨어와 프로세스의 보안을 책임진다. 한정된 시스템 자원을 효율적으로 관리하여 프로그램의 실행을 원활하게 한다.    
-Context Switching?    
    현재 진행하고 있는 Task(Process, Thread)의 상태를 저장하고 다음 진행할 Task의 상태 값을 읽어 적용하는 과정을 말합니다.


---


### 스와핑(Swapping)
프로그래밍 언어에서 스왑(Swap)이라는 단어는 두 개의 값을 바꾼다는 뜻을 가지고 있다. OS 에서의 의미도 이와 크게 다르지 않다.  
  
예를 들어 유저 공간(주기억장치)이 있는데 이 곳은 최대 10개의 프로세스(Process)를 올릴 수 있다고 가정하고 현재 메모리를 꽉 차지하고 있다.
이 상황에서 11번째 프로세스가 실행된다면 당연히 메모리가 가득 차서 올릴 수 없다. 그래서 10개의 프로세스 중에서 하나의 프로세스를 잠깐 내리고
그 사이에 11번째 프로세스를 실행시키는 방법이다. 어떤 하나의 프로세스가 이벤트가 발생하기까지 기다리고 있는데 1시간이 넘었다. 그렇다면
이 프로세는 앞으로도 쭉 기다릴 확률이 높다. 그렇다면 이 프로세스를 잠깐 이 공간(주기억장치)에서 쫓아내고 이 공간에 새로운 프로세스를 넣는 것이다.
당연히 쫓아낼 때 그냥 쫓아내면 안되고 잠깐 다른 곳에 저장을 해놔야한다. 그리고 그 다른 곳은 하드디스크, SSD 와 같은 Secondary storage(보조기억장치)가 된다.
이렇게 프로세스 단위로 쫓아내는 것을 __Swap out__ 이라고 한다.  

  
![A](imgs/os_swapping.PNG)  
  
시간이 지나고 아까 그 쫓아낸 프로세스 이벤트 요청이 왔다. 그럼 이 프로세스를 다시 메모리에 올려서 실행시키면 되고 이렇게 다시 메모리로 로딩하는 것을
__Swap in__ 이라고 한다. 이렇게 프로세스 단위로 Swap in, Swap out 하는 것을 Swapping 이라고 한다.  

>+ 하드디스크(보조기억장치)에 있던 것을 메모리에 다시 로딩하고 실행한다. 이렇게 교체 시스템에서 나타나는 Context Switching 시간은 생각보다 꽤 높다.
>+ 시간은 오래 걸리지만 부족한 메모리에 더 많은 프로세스를 실행할 수 있다는 큰 장점이 있다. 
>+ 이러한 Swapping 기법은 이후 가상 메모리 관리 방법인 페이징 기법으로 발전했다.


  
### 메모리 단편화(Memory Fragmentation)
주기억장치에서 메모리의 공간이 작은 조각으로 나누어져, __사용하기에 충분한 양의 메모리가 존재는 하지만 사실상 사용이 불가능한 경우__ 메모리 단편화가 발생했다고 한다.
이러한 메모리 단편화는 내부 단편화(Internal Fragmentation)와 외부 단편화(External Fragmentation)로 구분할 수 있다.  
  
  
#### 내부 단편화(Internal Fragmentation)
> 필요한 양보다 더 큰 메모리가 할당이 되어 할당 된 메모리 내부에 사용하는 메모리 공간 이외에 사용하지 않는 메모리 공간이 발생했을 때를 말한다.  

#### 외부 단편화(External Fragmentation)
> 메모리가 할당이 되고 해제가 되는 작업이 반복될 때 작은 단위의 메모리가 띄엄띄엄 존재하게 된다. 이 때, 비어있는 메모리의 공간은 충분한 양이지만 실제로 사용할 수 없는 경우를 말한다.  
  
![A](imgs/os_fragmentation.JPG)  
  
  
이러한 메모리 단편화를 해결하는 방법에는 여러가지가 있다.

#### 압축(Compaction) 기법
메모리 공간들을 하여, 단편화로 인해  __분산되어 있는 메모리 공간들을 하나로 합치는 기법__ 이다.

#### 통합(Coalescing) 기법
단편화로 인해 __분산된 메모리 공간들을 인접해 있는 것끼리 통합시켜 큰 메모리 공간으로 합치는 기법__ 이다.

>압축기법과 통합기법의 차이는 압축은 재배치가 일어나지만 통합은 인접한 공간끼리 통합된다는 차이가 있다. 
하지만 이렇게 비어 있는 공간을 연속적인 공간으로 만들고 움직이는 작업은 메모리를 Copy 해야하고 이 시점에서 반드시
I/O Problem 이 발생하게 된다. 하드디스크가 병목되기 때문에 좋은 방법은 아니다. 이는 결국 페이징(Paging)의 배경이 된다.


### 페이징(Paging) 기법과 세그멘테이션(Segmentation) 기법
> __가상메모리__
>> 우선 가상 메모리를 관리하는 기법인 페이징기법과 세그멘테이션기법을 살펴보기전에 가상메모리에 대해 알아보자. 가상 메모리는 메모리에 로드된 즉, 실행중인 프로세스가 가상의 공간을 참조하여 마치 커다란 물리메모리를 갖고 있는 것처럼 착각하게 만드는 것이다.
이러한 가상 메모리는 각 프로세스 당 메인 메모리와 동일한 크기로 하나씩 할당되며 그 공간은 보조기억장치(HDD, SSD) 공간을 이용한다. 프로세스의 일부만 메모리에 로드하고 나머지는 보조기억장치에 두는 형태이다.
이렇게 할당이 되면 MMU(메모리 관리 장치로 CPU 코어 안에 탑재되어서 가상 주소를 실제 메모리 주소로 변환해주는 장치) 에 의해 물리 주소로 변환이 된다.
MMU 를 더 설명하기에는 내용이 길어지기에 생략하기로 한다. 다만 프로세스를 동작시키기 위해서는 가상 주소가 아닌 실제 물리 주소가 필요하고 가상 메모리에 있는 가상 주소를 물리 주소로 변환시켜주는 장치라고 이해하고 넘어가자.
 
#### 페이징(Paging) 기법
![A](imgs/os_external_fragmentation.PNG)
위 외부 단편화 발생 상황을 보면 프로세스가 비어있는 메모리(Hole)보다 크기 때문에 발생한다. 그리고 이 Hole 의 크기도 제각각,
프로세스의 크기도 제각각이다. 그래서 이러한 외부 단편화를 완벽히 발생시키지 않기 위한 방법이 페이징(Paging) 기법이다.
페이징은 Logical Address(프로세스) 를 동일한 크기로 자르고, Physical Address(메모리) 도 이것과 동일한 크기로 자른다(Frame 단위).
그리고 개별 페이지는 순서에 상관없이 Physical Memory 에 있는 프레임에 Mapping 되어 저장된다고 볼 수 있다. 그림을 보면 조금 더 쉽게 이해가 된다.
![A](imgs/os_paging.PNG)
이렇게 하면 하나의 프로세스를 굳이 연속적으로 배치할 필요가 없다. 그래서 Paging 을 통해 외부단편화 문제를 해결할 수 있다.  
  
하지만 페이징(Paging)은 __내부 단편화 문제를 해결할 수는 없다.__  
프로세스의 크기가 나누어져있는 Page 의 크기에 딱 맞게끔 나누어진다는 보장이 없기 때문이다.
![A](imgs/os_paging_internal_fragmentation_problem.JPG)
한 페이지의 크기가 7K 일때 45K 의 프로세스를 실행한다고 하면 3K의 내부 단편화가 발생할 수 밖에 없다.
그럼 페이지의 크기를 더 잘게 쪼개면 되지 않느냐고 할 수 있다. 
하지만 페이지의 크기가 너무 작아질수록(혹은 너무 커질수록) 발생되는 문제점들이 있다.
> 1. 페이지 단편화 감소(증가)
> 2. 한 개의 페이지를 주기억장치로 이동하는 시간이 줄어든다.(늘어남)
> 3. 프로세스 수행에 필요한 내용만 주기억장치에 적재 가능(불가능)
> 4. 기억장치 효율성이 높아짐(낮아짐)
> 5. 페이지 맵 테이블의 크기가 커짐(작아짐)
> 6. Mapping 속도가 늦어진다.(빨라짐) 
> 7. 입/출력 시간의 증가(감소)

#### 세그멘테이션(Segmentation) 기법
Paging 기법은 외부 단편화 문제를 해결할 수 있었지만, 내부 단편화 발생의 문제가 있었다. 그래서 내부 단편화 문제를 해결하기 위한 방법이 세그멘테이션(Segmentation) 기법이다.
프로세스를 일정한 단위인 페이지 단위로 잘랐다면 물리적인 단위인 세그먼트 단위로 자르는 것이 세그멘테이션 기법이다.  
  
세그멘테이션은 프로세스를 세그먼트(서로 다른 크기를 가지는 논리적인 블록이 연속적인 공간에 배치되어 있는것)의 집합으로 생각한다.  
잠깐 세그먼트가 무엇인지 그림으로 보고 이해하자.
![A](imgs/os_segmetation_segment.PNG)
이렇게 각각 나누어진 영역을 세그먼트라고 부른다. 그리고 이렇게 영역별로 쪼개는 기술을 세그멘테이션이라고 한다.
  
사실 하나의 프로세스가 동작하기 위해서는 기본적으로
코드, 데이터, 스택 세 가지의 세그먼트는 항상 가지고 있다. 그리고 더 들어가 본다면 코드에서도 main 함수가 있고 다른 함수들 또한 존재할 수 있다.
데이터를 본다고 해도 어떠한 구조체가 존재할 수도, 배열이 있을 수도 있다. 그래서 세그멘테이션은 물리적인 크기의 단위가 아닌 논리적 내용의 단위로 자르기 때문에 
세그먼트들의 크기는 일반적으로는 같지 않다.  
  
세그멘테이션을 사용하면 세그먼트 테이블(Segment Table)이라고 세그먼트의 시작주소(base)와 세그먼트의 길이(limit) 정보가 담긴 테이블을 운용한다.
이로 인해 각각의 세그먼트를 offset 단위로 관리할 수 있다.
![A](imgs/os_segmentation.png)

MMU(CPU 코어 안에 탑재되어서 가상 주소를 실제 메모리 주소로 변환해주는 장치) 내의 재배치 레지스터를 이용하여 논리 주소를 물리 주소로 바꾸어 주는 방식을 취한다.
MMU 는 세그먼트 테이블로 CPU 에서 할당한 논리 주소에 해당하는 물리 주소의 위치를 가지고 있다. 세그먼트는 세그먼트 테이블에 연속적으로 저장되기 때문에 CPU 는 프로세스가 연속된 메모리 공간에 위치한다고 착각을 하게 된다.
하지만 세그멘테이션 기법은 동적 할당(세그먼트들의 크기가 다르기 때문에 미리 분할해 둘 수 없고 그때 그때 빈 공간을 찾아 적재)이기 때문에 외부 단편화가 발생할 수 있다.  
  
#### 페이징(Paging) + 세그멘테이션(Segmentation)
페이징은 내부 단편화 문제를 가지고 있고 세그멘테이션은 외부 단편화 문제를 가지고 있다. 그래서 두 방식을 모두 사용하여 장점만을 가져오고자 한다.
따라서 세그멘트를 페이징하는 방식을 취한다.
![A](imgs/os_paging_and_segmentation.jpg)
우선 프로세스를 처음에 세그먼트 단위로 자른다. 의미 있는 단위로 나누게 되면 정보 보호와 공유를 하는 측면에서 세그먼트의 이점을 가져오면서 내부단편화를 막는다. 하지만 외부단편화가 발생할 수 있기 때문에
잘라진 세그먼트를 다시 일정 간격인 페이지 단위로 자르는 페이징 기법을 취한다. 그래서 메모리에 적재하게 되면 페이징의 일정 단위로 다시 잘렸기 때문에 외부 단편화가 발생하지 않는다. 하지만 이와 같은 경우에는 세그먼트 테이블과 페이징 테이블 두 가지를 모두 겨쳐야하므로 속도 면에서는 조금 떨어질 수 있다.
실제로 x86 메모리 관리를 이런 식으로 두 가지 기법을 섞어 작업을 처리한다.



 ---

##### 출처

`메모리 관리 :`
https://dheldh77.tistory.com/entry/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC-%EC%A0%84%EB%9E%B5Memory-Management-Strategy<br>
https://velog.io/@codemcd/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9COS-12.-%EC%A3%BC%EA%B8%B0%EC%96%B5%EC%9E%A5%EC%B9%98%EA%B4%80%EB%A6%AC<br>
`캐시의 지역성 :`
https://k39335.tistory.com/38<br>
`가상 메모 :`
https://velog.io/@pa324/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C-%EA%B0%80%EC%83%81-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B0%9C%EB%85%90-4dk2q3ivff



